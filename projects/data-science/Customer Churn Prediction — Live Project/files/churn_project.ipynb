{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89403bc9",
   "metadata": {},
   "source": [
    "\n",
    "# Customer Churn Prediction â€” Live Project (Shiksha Skills Institute)\n",
    "\n",
    "**Goal:** Build an end-to-end machine learning pipeline to predict customer churn for a telecom company and deploy a simple web app.\n",
    "\n",
    "**You will:**\n",
    "1. Load and clean the Telco Customer Churn dataset.\n",
    "2. Explore data and visualize key patterns.\n",
    "3. Build and evaluate ML models (Logistic Regression, Random Forest).\n",
    "4. Create and persist a production-ready pipeline.\n",
    "5. (Optional) Run a Streamlit app for live predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Setup\n",
    "> Before running:\n",
    "> - Place the dataset CSV in this folder.\n",
    "> - Common filenames (from Kaggle): `Telco-Customer-Churn.csv` OR `WA_Fn-UseC_-Telco-Customer-Churn.csv`.\n",
    "> - Install dependencies from `requirements.txt` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f124c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core libs\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization (No seaborn, per project constraints)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Utility\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "print(\"Setup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf53d41",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Load Data\n",
    "We'll try to load from one of two common filenames. If not found, raise a helpful error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb7c620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidate_files = [\n",
    "    \"Telco-Customer-Churn.csv\",\n",
    "    \"WA_Fn-UseC_-Telco-Customer-Churn.csv\"\n",
    "]\n",
    "\n",
    "csv_path = None\n",
    "for f in candidate_files:\n",
    "    if os.path.exists(f):\n",
    "        csv_path = f\n",
    "        break\n",
    "\n",
    "if csv_path is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Dataset not found. Please place 'Telco-Customer-Churn.csv' or \"\n",
    "        \"'WA_Fn-UseC_-Telco-Customer-Churn.csv' in this folder and rerun.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"Loaded: {csv_path}, shape={df.shape}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e272e9f",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Quick Data Audit & Cleaning\n",
    "- Fix types (e.g., `TotalCharges` sometimes comes as string).\n",
    "- Handle missing values.\n",
    "- Strip whitespace in column names and string entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2215a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardize column names\n",
    "df.columns = [c.strip().replace(' ', '_') for c in df.columns]\n",
    "\n",
    "# Strip whitespace for object columns\n",
    "for c in df.select_dtypes(include=['object']).columns:\n",
    "    df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "# Convert TotalCharges to numeric if present\n",
    "if 'TotalCharges' in df.columns:\n",
    "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "\n",
    "# Basic info\n",
    "display(df.info())\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "# Missing values\n",
    "na_counts = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"Missing values per column:\\n\", na_counts[na_counts>0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc5b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simple imputation strategies:\n",
    "# - For numeric columns: fill with median\n",
    "# - For categorical columns: fill with most frequent (mode)\n",
    "\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for c in num_cols:\n",
    "    df[c] = df[c].fillna(df[c].median())\n",
    "\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].fillna(df[c].mode()[0])\n",
    "\n",
    "print(\"After imputation:\", df.isna().sum().sum(), \"missing values remaining.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b148e57",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Exploratory Data Analysis (EDA)\n",
    "We'll look at churn distribution and key drivers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure target column name 'Churn' exists; convert to binary 1/0 if necessary\n",
    "target_col = None\n",
    "for c in df.columns:\n",
    "    if c.lower() == 'churn':\n",
    "        target_col = c\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    raise KeyError(\"Target column 'Churn' not found. Please ensure the dataset has a 'Churn' column.\")\n",
    "\n",
    "# Map Yes/No to 1/0 if needed\n",
    "if df[target_col].dtype == 'object':\n",
    "    df[target_col] = df[target_col].map({'Yes':1, 'No':0}).fillna(df[target_col]).astype(int)\n",
    "\n",
    "# Basic churn rate\n",
    "churn_rate = df[target_col].mean()\n",
    "print(f\"Churn rate: {churn_rate:.3f}\")\n",
    "\n",
    "# Plot churn count\n",
    "plt.figure()\n",
    "df[target_col].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Churn Count (0=No, 1=Yes)')\n",
    "plt.xlabel('Churn')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example: relationship between Contract and Churn (if Contract exists)\n",
    "if 'Contract' in df.columns:\n",
    "    cross = pd.crosstab(df['Contract'], df[target_col], normalize='index')\n",
    "    display(cross)\n",
    "\n",
    "    plt.figure()\n",
    "    cross.plot(kind='bar', rot=45)\n",
    "    plt.title('Churn Rate by Contract Type')\n",
    "    plt.xlabel('Contract Type')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.legend(title='Churn')\n",
    "    plt.show()\n",
    "\n",
    "# Example: tenure distribution by churn (if Tenure exists)\n",
    "if 'tenure' in df.columns:\n",
    "    plt.figure()\n",
    "    df[df[target_col]==0]['tenure'].plot(kind='hist', alpha=0.5, bins=30)\n",
    "    df[df[target_col]==1]['tenure'].plot(kind='hist', alpha=0.5, bins=30)\n",
    "    plt.title('Tenure Distribution by Churn')\n",
    "    plt.xlabel('Tenure (months)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918dd35",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Train/Test Split & Preprocessing\n",
    "We will:\n",
    "- Split the data\n",
    "- One-hot encode categorical features\n",
    "- Scale numeric features (for linear models)\n",
    "- Build pipelines for Logistic Regression and Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f790599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define X, y\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Identify column types\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocess: OneHot for categoricals, Scale numerics (benefits LR)\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pipelines\n",
    "pipe_lr = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=200))\n",
    "])\n",
    "\n",
    "pipe_rf = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', RandomForestClassifier(n_estimators=300, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288855ac",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train & Evaluate Models\n",
    "We'll compare Logistic Regression and Random Forest on:\n",
    "- Accuracy, Precision, Recall, F1\n",
    "- ROC AUC\n",
    "- Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b66b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(pipe, name):\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_test)\n",
    "    probs = None\n",
    "    if hasattr(pipe, \"predict_proba\"):\n",
    "        probs = pipe.predict_proba(X_test)[:, 1]\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_test, preds, digits=3))\n",
    "    if probs is not None:\n",
    "        auc = roc_auc_score(y_test, probs)\n",
    "        print(f\"ROC AUC: {auc:.3f}\")\n",
    "        RocCurveDisplay.from_predictions(y_test, probs)\n",
    "        plt.title(f'ROC Curve â€” {name}')\n",
    "        plt.show()\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    plt.figure()\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix â€” {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    for (i, j), v in np.ndenumerate(cm):\n",
    "        plt.text(j, i, str(v), ha='center', va='center')\n",
    "    plt.show()\n",
    "    return pipe\n",
    "\n",
    "model_lr = evaluate_model(pipe_lr, \"Logistic Regression\")\n",
    "model_rf = evaluate_model(pipe_rf, \"Random Forest\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3a3dca",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Select & Persist the Best Pipeline\n",
    "For simplicity, we will persist the Random Forest pipeline (often stronger on tabular data).  \n",
    "This saved file can be used directly by the Streamlit app.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b29fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_pipeline = model_rf  # choose based on above evaluation\n",
    "save_path = \"churn_pipeline.pkl\"\n",
    "joblib.dump(best_pipeline, save_path)\n",
    "print(f\"Saved trained pipeline to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e94a4",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Try a Single Prediction (Sanity Check)\n",
    "Create a single-row DataFrame mimicking a customer record and predict churn probability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8531fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a template record from X_test to preserve columns\n",
    "sample = X_test.iloc[[0]].copy()\n",
    "\n",
    "# Predict\n",
    "proba = best_pipeline.predict_proba(sample)[0,1]\n",
    "pred = best_pipeline.predict(sample)[0]\n",
    "print(\"Sample prediction:\", pred, \" | Probability of churn:\", round(proba, 3))\n",
    "\n",
    "sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641314f6",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Next Steps\n",
    "- Iterate on feature engineering (e.g., new features like tenure buckets, TotalSpend).\n",
    "- Hyperparameter tuning (GridSearchCV) for further gains.\n",
    "- Ship the Streamlit app using the `churn_pipeline.pkl` saved here.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
